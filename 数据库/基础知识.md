## drop、delete 与 truncate 区别

### 1. 操作对象与作用

- **`DELETE`**：属于 **DML（数据操纵语言）**，用于删除表中**部分或全部行**（可通过 `WHERE` 条件指定删除范围），**不影响表结构**（表依然存在，索引、约束、字段等保留）。
  例：`DELETE FROM students WHERE age < 18;`（仅删除年龄小于 18 的学生）。
- **`TRUNCATE`**：属于 **DDL（数据定义语言）**，用于**清空表中所有数据**（无法通过 `WHERE` 条件筛选，只能删除全表数据），但**保留表结构**（字段、索引、约束等依然存在）。
  例：`TRUNCATE TABLE students;`（删除 `students` 表中所有数据，但表本身还在）。
- **`DROP`**：属于 **DDL（数据定义语言）**，用于**删除整个表**（包括表结构、所有数据、索引、约束等），表会从数据库中彻底消失。
  例：`DROP TABLE students;`（`students` 表不再存在）。

### 2. 事务与回滚

- **`DELETE`**：是 DML 操作，会被事务管理，**支持回滚**（若未提交事务，可通过 `ROLLBACK` 恢复删除的数据）。
  例：开启事务后执行 `DELETE`，未 `COMMIT` 前可回滚。
- **`TRUNCATE`**：是 DDL 操作，执行后会**自动提交事务**，**不支持回滚**（删除后无法恢复数据）。
- **`DROP`**：是 DDL 操作，执行后**自动提交事务**，**不支持回滚**（表删除后无法恢复，需通过备份重建）。

### 3. 执行速度

- **`TRUNCATE`**：速度最快。因为它不记录每行数据的删除日志，仅通过重建表结构（逻辑上）清空数据（比 `DROP + CREATE` 更高效）。
- **`DELETE`**：速度较慢。因为会逐行删除数据并记录详细日志（用于回滚），数据量越大，速度越慢。
- **`DROP`**：速度快（删除表结构），但与 `TRUNCATE` 场景不同（`DROP` 是删除表，`TRUNCATE` 是清空数据）。

### 4. 对自增列的影响

- **`DELETE`**：不会重置自增列（如 MySQL 的 `AUTO_INCREMENT`）。例如，表中自增 ID 最大为 100，`DELETE` 后插入新数据，ID 会从 101 开始。
- **`TRUNCATE`**：会重置自增列。清空表后插入新数据，ID 会从初始值（通常是 1）开始。
- **`DROP`**：删除表后，自增列随表结构一起消失（若重建表，自增列需重新定义）。

### 5. 适用场景

- **`DELETE`**：需删除部分数据（带 `WHERE` 条件），或需支持回滚时使用。
- **`TRUNCATE`**：需清空全表数据（无需保留部分记录），且追求效率、不担心回滚时使用（如测试环境清空数据）。
- **`DROP`**：需彻底删除表（包括结构）时使用（如废弃的表）。

### 总结对比表

| 特性             | `DELETE`            | `TRUNCATE`       | `DROP`                |
| ---------------- | ------------------- | ---------------- | --------------------- |
| 操作对象         | 表中部分 / 全部数据 | 表中全部数据     | 整个表（结构 + 数据） |
| 是否支持 `WHERE` | 是                  | 否               | 否                    |
| 事务回滚         | 可回滚（DML）       | 不可回滚（DDL）  | 不可回滚（DDL）       |
| 自增列重置       | 不重置              | 重置             | 随表删除              |
| 日志记录         | 记录每行日志        | 不记录详细日志   | 记录表删除日志        |
| 速度             | 慢（逐行删除）      | 快（重建表逻辑） | 快（删除表结构）      |
| 对表结构影响     | 无（保留结构）      | 无（保留结构）   | 删除表结构            |

## 数据库设计通常分为哪几步

### 1. **需求分析（基础阶段）**

**目的**：明确数据库需要解决的业务问题，收集并梳理所有需求。
**核心工作**：



- 与业务方（用户、产品、运营等）沟通，明确**数据需求**（需要存储哪些数据，如用户信息、订单信息）、**处理需求**（需要支持哪些操作，如查询、新增、统计）、**约束条件**（如数据格式、业务规则、性能要求）。
- 输出《需求规格说明书》，包含：
    - 实体清单（如 “用户”“商品”“订单”）；
    - 实体属性（如用户的 “ID”“姓名”“手机号”）；
    - 业务流程（如 “下单 - 支付 - 发货” 的数据流）；
    - 性能指标（如并发量、查询响应时间）。



**关键**：需求分析是后续所有设计的基础，务必全面、准确，避免遗漏核心业务场景。

### 2. **概念结构设计（抽象阶段）**

**目的**：将需求转化为**独立于具体数据库系统**的抽象概念模型（不涉及具体表结构或数据库类型）。
**核心工作**：



- 基于需求分析，提炼**实体（Entity）**（如 “用户”“订单”）、**属性（Attribute）**（实体的特征，如订单的 “订单号”“金额”）、**实体间的联系（Relationship）**（如 “用户 - 订单” 是 “一对多” 联系：一个用户可创建多个订单）。
- 用**E-R 图（实体 - 关系图）** 可视化概念模型，清晰展示实体、属性、联系（如 1:1、1:n、m:n）。



**示例**：



- 实体 “学生” 属性：学号、姓名、专业；
- 实体 “课程” 属性：课程号、课程名、学分；
- 联系 “选课”：学生与课程是 m:n 联系（一个学生可选多门课，一门课可被多个学生选），联系属性为 “成绩”。



**关键**：概念模型需简洁、抽象，不依赖具体数据库（如 MySQL、Oracle），聚焦业务本质。

### 3. **逻辑结构设计（转化阶段）**

**目的**：将概念模型（E-R 图）转化为**具体数据库系统支持的逻辑模型**（如关系模型中的表结构），并优化结构。
**核心工作**：



- 模型转化

  ：将 E-R 图中的实体、联系转化为关系表：

    - 实体→表（实体名→表名，属性→列名）；
    - 1:1 联系→可合并到任意一方表中（如 “用户” 与 “用户详情” 是 1:1，可将详情属性合并到 “用户表”）；
    - 1:n 联系→在 “多” 的一方表中添加 “一” 方的主键作为外键（如 “订单表” 添加 “用户 ID” 关联 “用户表”）；
    - m:n 联系→新增中间表（如 “学生 - 课程” 的 m:n 联系，新增 “选课表”，含 “学号”“课程号”“成绩”）。

- **规范化处理**：通过**范式（Normal Form）** 优化表结构，减少数据冗余和操作异常（如 1NF 确保列不可再分，2NF 消除部分依赖，3NF 消除传递依赖）。

- 确定数据类型、约束（主键、外键、非空、唯一、默认值等）。



**输出**：逻辑模型（表结构设计表），含表名、列名、数据类型、约束、主键、外键等。

### 4. **物理结构设计（落地阶段）**

**目的**：根据逻辑模型，结合具体数据库系统（如 MySQL、PostgreSQL），设计**物理存储结构**，优化性能。
**核心工作**：



- **选择存储引擎**：如 MySQL 中，InnoDB 支持事务和外键，适合核心业务表；MyISAM 查询快但不支持事务，适合只读表（如日志表）。
- **设计索引**：为高频查询字段（如订单号、用户 ID）创建索引，提升查询效率；避免过度索引（影响写入性能）。
- **确定存储细节**：表空间分配、数据文件路径、分区策略（如大表按时间分区）、缓存设置等。
- **性能优化**：如分表分库（应对海量数据）、字段长度优化（如手机号用 CHAR (11) 而非 VARCHAR）。



**关键**：物理设计需结合具体数据库特性和业务性能需求，平衡读写效率。

### 5. **数据库实施（开发阶段）**

**目的**：将设计落地为实际可运行的数据库。
**核心工作**：



- 编写 SQL 脚本，创建数据库、表、索引、视图、存储过程等（如`CREATE DATABASE`、`CREATE TABLE`）。
- 导入测试数据，验证表结构、约束、关联关系是否符合设计（如外键是否生效、索引是否提升查询速度）。
- 进行功能测试（如新增 / 查询数据是否正常）、性能测试（如并发写入是否卡顿），修复设计缺陷。

### 6. **运行与维护（迭代阶段）**

**目的**：保障数据库长期稳定运行，并根据业务变化持续优化。
**核心工作**：



- **日常维护**：监控数据库性能（如 CPU、内存、磁盘占用）、备份数据（定时全量 + 增量备份）、处理故障（如死锁、数据损坏）。
- **优化迭代**：根据业务增长（如数据量激增）或需求变化（如新增字段、表），调整结构（如新增索引、分表）、重构表（如拆分大表）。
- **安全管理**：权限控制（如只读用户、读写用户）、防注入、数据加密等。

## UTF-8 使用

UTF-8 是目前最常用的 Unicode 字符编码方式，它通过可变长度字节（1-4 字节）表示 Unicode 字符集中的字符，兼容 ASCII 编码（1 字节部分与 ASCII 完全一致），且支持全球几乎所有语言（包括中文、英文、日文、符号等），因此被广泛用于各类场景。

UTF-8 使用 1 到 4 个字节为每个字符编码， UTF-16 使用 2 或 4 个字节为每个字符编码，UTF-32 固定位 4 个字节为每个字符编码。UTF-8 可以根据不同的符号自动选择编码的长短，像英文字符只需要 1 个字节就够了，这一点 ASCII 字符集一样 。因此，对于英语字符，UTF-8 编码和 ASCII 码是相同的。UTF-32 的规则最简单，不过缺陷也比较明显，对于英文字母这类字符消耗的空间是 UTF-8 的 4 倍之多

**注意**

MySQL 中的 utf8 实际是 utf8mb3，仅支持 3 字节字符（不支持 emoji），需用 utf8mb4（真正的 UTF-8，支持 4 字节）：


## Mysql中的数据排序是怎么实现的

### 1. 基础语法








```sql
SELECT 列1, 列2
FROM 表名
ORDER BY 列1 [ASC|DESC], 列2 [ASC|DESC];
```

- `ASC`（默认）代表升序排列。
- `DESC` 表示降序排列。
- 可以按照多个列进行排序，并且各列能使用不同的排序方向。

### 2. 示例

假设有一个名为 `students` 的表，其内容如下：

| id   | name  | score | age  |
| ---- | ----- | ----- | ---- |
| 1    | Alice | 85    | 20   |
| 2    | Bob   | 92    | 19   |
| 3    | Carol | 85    | 22   |

#### 按单字段升序排列





```sql
SELECT * FROM students ORDER BY score;
```

结果会按照 `score` 列从小到大排序：

| id   | name  | score | age  |
| ---- | ----- | ----- | ---- |
| 1    | Alice | 85    | 20   |
| 3    | Carol | 85    | 22   |
| 2    | Bob   | 92    | 19   |

#### 按多字段排序









```sql
SELECT * FROM students ORDER BY score DESC, age ASC;
```

结果会先按照 `score` 列从大到小排序，若 `score` 相同，则按 `age` 列从小到大排序：

| id   | name  | score | age  |
| ---- | ----- | ----- | ---- |
| 2    | Bob   | 92    | 19   |
| 1    | Alice | 85    | 20   |
| 3    | Carol | 85    | 22   |

### 3. 排序优化

#### 索引优化

当排序的字段有索引时，MySQL 可以直接利用索引的有序性来避免额外的排序操作，从而提升查询速度。





```sql
-- 为 score 列创建索引
CREATE INDEX idx_score ON students (score);
```

#### 临时表与文件排序

要是排序操作无法利用索引，MySQL 可能会把数据放到临时表中，然后进行文件排序（filesort），这种方式在处理大量数据时效率较低。你可以通过 `EXPLAIN` 命令来查看查询是否使用了文件排序：







```sql
EXPLAIN SELECT * FROM students ORDER BY score;
```

### 4. 注意事项

- **NULL 值处理**：在 MySQL 中，`NULL` 值会被当作最小值。在升序排列时，`NULL` 值会排在最前面；在降序排列时，`NULL` 值会排在最后面。
- **排序规则**：对于字符串排序，会受到字符集和排序规则（如 `utf8mb4_general_ci`）的影响，其中 `_ci` 表示不区分大小写。
- **性能影响**：对大量数据进行排序时，要尽量利用索引，或者限制返回的数据量（使用 `LIMIT`）。


## ChangeBuffer是什么，作用是什么

### 1. 基本概念

- **适用场景**：Change Buffer 适用于非唯一的二级索引（Secondary Index），并且这些索引页不在缓冲池（Buffer Pool）中。
- **工作方式**：当对这类索引进行写操作（INSERT、UPDATE、DELETE）时，MySQL 不会立即更新磁盘上的索引页，而是先把这些变更记录到 Change Buffer 中，之后在索引页被读取到缓冲池时，再进行合并（Merge）操作，将变更应用到索引页上。

### 2. 核心作用

#### 减少磁盘随机 I/O

传统的索引更新需要立即读取磁盘上的索引页，这会产生大量随机 I/O。而 Change Buffer 把多个写操作合并为一次，将随机写转化为顺序写，从而显著提升了写入性能。

#### 提升批量写入效率

在进行批量插入（如导入数据）时，Change Buffer 会缓存所有的索引变更，等到系统空闲或者索引页被读取时再统一处理，这样能极大地加快写入速度。

#### 优化读性能

虽然 Change Buffer 主要是为写操作优化的，但它对读操作也有间接的好处。因为变更被缓存在内存中，后续的读操作可以直接获取最新的数据，无需等待磁盘更新。

### 3. 工作流程

1. **写入操作**：当执行 INSERT、UPDATE 或 DELETE 语句时，如果涉及的非唯一索引页不在缓冲池中，MySQL 会将变更记录到 Change Buffer 中。

2. 合并操作

   ：以下情况会触发 Change Buffer 与索引页的合并：

  - 索引页被读取到缓冲池时。
  - 后台线程定期进行合并。
  - 执行 CHECK TABLE、OPTIMIZE TABLE 等命令时。

3. **持久化**：合并后的索引页会在合适的时机刷新到磁盘。

### 4. 相关参数

#### `innodb_change_buffer_max_size`

- 作用：控制 Change Buffer 占用缓冲池的最大百分比，默认值为 25%，最大值为 50%。
- 调整建议：对于写密集型的工作负载，可以适当提高该值（如 40%）；对于读密集型的工作负载，则可以降低该值。

#### `innodb_change_buffering`

- 作用：控制哪些操作会被缓存到 Change Buffer 中，支持的值有 `all`（默认）、`inserts`、`deletes`、`changes`、`none`。
- 示例：如果只需要缓存插入操作，可以设置为 `inserts`。

### 5. 适用场景与限制

#### 适用场景

- **写密集型应用**：像日志记录、批量数据导入等场景。
- **非唯一索引较多的表**：例如有大量分类、标签的内容表。
- **磁盘 I/O 成为瓶颈的系统**：Change Buffer 能有效减少随机 I/O。

#### 限制

- **仅适用于 InnoDB 引擎**：MyISAM 等其他引擎不支持 Change Buffer。
- **不适用于唯一索引**：因为唯一索引需要在插入时立即检查唯一性，必须读取索引页，所以无法使用 Change Buffer。
- **可能导致查询延迟**：如果 Change Buffer 中的数据长时间未合并，在读取相关索引页时，可能会因为合并操作而导致查询延迟增加。

### 6. 监控与优化

#### 监控指标










```sql
-- 查看 Change Buffer 的使用情况
SHOW ENGINE INNODB STATUS;

-- 关注以下指标：
-- buffer pool size：缓冲池总大小
-- free buffers：空闲缓冲块数量
-- database pages：数据页数量
-- old database pages：旧数据页数量
-- modified db pages：已修改但未刷新到磁盘的页数
-- pending reads：等待读取的页数
-- pending writes (flush)：等待刷新的页数
-- pages made young：因访问而变为年轻的页数
-- pages not made young：未因访问而变为年轻的页数
-- youngs/s：每秒年轻页数变化率
-- non-youngs/s：每秒非年轻页数变化率
-- pages read：读取的页数
-- pages created：创建的页数
-- pages written：写入的页数
-- reads/s：每秒读取速率
-- creates/s：每秒创建速率
-- writes/s：每秒写入速率
-- buffer pool hit rate：缓冲池命中率
-- young-making rate：年轻页生成率
-- not (young-making rate)：非年轻页生成率
-- adaptive hash index hits：自适应哈希索引命中率
-- pending reads adaptive hash index：等待自适应哈希索引读取的页数
-- adaptive hash index writes：自适应哈希索引写入次数
-- page split ：页分裂次数
```

#### 优化建议

- **定期执行 `OPTIMIZE TABLE`**：对于变更频繁的表，定期执行该命令可以重建索引，减少 Change Buffer 的使用。
- **调整合并频率**：通过 `innodb_max_dirty_pages_pct` 参数控制脏页比例，从而影响合并频率。
- **监控内存使用**：确保 Change Buffer 不会占用过多的缓冲池内存，以免影响数据页的缓存效果。

### 7. Change Buffer 与其他缓存的对比

| 缓存类型      | 适用对象       | 主要作用         |
| ------------- | -------------- | ---------------- |
| Change Buffer | 非唯一二级索引 | 减少写操作的 I/O |
| Buffer Pool   | 数据页和索引页 | 缓存常用数据     |
| Query Cache   | 查询结果       | 加速重复查询     |
| Key Buffer    | MyISAM 索引页  | 加速 MyISAM 读   |


## Mysql中的int(11)中的11表示什么

### 1. 显示宽度的概念

- **仅影响显示**：`INT(11)` 里的 `11` 规定了在使用 `ZEROFILL` 填充时，数值显示的最少位数。若数值的位数不足，会在左侧用 `0` 进行填充。
- **不限制存储范围**：不管显示宽度设定为多少，`INT` 类型始终占用 **4 个字节**，其存储范围固定为 `-2147483648` 到 `2147483647`（有符号）或者 `0` 到 `4294967295`（无符号）。

### 2. 示例说明

#### 无 ZEROFILL 的情况











```sql
CREATE TABLE test (
  num1 INT(3),      -- 显示宽度为 3
  num2 INT(11)      -- 显示宽度为 11（默认）
);

INSERT INTO test VALUES (1, 1);

SELECT num1, num2 FROM test;
+------+------+
| num1 | num2 |
+------+------+
| 1    | 1    |
+------+------+
```

此时，`INT(3)` 和 `INT(11)` 显示结果相同，显示宽度参数不产生实际影响。

#### 有 ZEROFILL 的情况








```sql
CREATE TABLE test (
  num1 INT(3) ZEROFILL,  -- 不足 3 位时用 0 填充
  num2 INT(11) ZEROFILL  -- 不足 11 位时用 0 填充
);

INSERT INTO test VALUES (1, 1);

SELECT num1, num2 FROM test;
+------+-------------+
| num1 | num2        |
+------+-------------+
| 001  | 00000000001 |
+------+-------------+
```

- `num1` 显示为 `001`（3 位）。
- `num2` 显示为 `00000000001`（11 位）。

### 3. 关键要点

#### 与存储无关

- 不管是 `INT(1)` 还是 `INT(20)`，它们占用的存储空间都是 4 字节，存储范围也完全一样。
- 显示宽度纯粹是为了格式化输出，对数值的存储和计算没有任何作用。

#### ZEROFILL 的隐含效果

- 当使用 `ZEROFILL` 时，MySQL 会自动为该列添加 `UNSIGNED` 属性，这意味着该列不能存储负数。



```sql
CREATE TABLE test (
  num INT(5) ZEROFILL  -- 等价于 INT(5) UNSIGNED ZEROFILL
);

INSERT INTO test VALUES (-1);  -- 报错：Out of range value for column 'num'
```

#### 默认显示宽度

- 对于不同的整数类型，MySQL 有对应的默认显示宽度：

  | 类型      | 占用字节 | 默认显示宽度 |
    | --------- | -------- | ------------ |
  | TINYINT   | 1        | 4            |
  | SMALLINT  | 2        | 6            |
  | MEDIUMINT | 3        | 9            |
  | INT       | 4        | 11           |
  | BIGINT    | 8        | 20           |

### 4. 实际应用建议

#### 不建议使用显示宽度

- 在大多数情况下，不需要手动指定显示宽度，直接使用 `INT` 即可。
- 显示宽度的功能可以在应用层（如代码中）实现，没必要依赖数据库。

#### 替代方案

- **在应用层格式化**：例如在 Python 中使用 `"{:011d}".format(1)` 进行格式化。
- **使用 CHAR 类型**：如果确实需要固定长度的数字字符串，可以考虑使用 `CHAR(11)` 类型。

### 5. 常见误区

#### 误区一：`INT(11)` 比 `INT(4)` 占用更多空间

事实并非如此，所有 `INT` 类型都占用 4 字节，显示宽度不会影响存储空间的大小。

#### 误区二：`INT(1)` 只能存储 1 位数字

`INT(1)` 依然可以存储完整范围的整数，只是显示时可能会根据宽度进行填充。

#### 误区三：显示宽度会限制输入长度

显示宽度不会对输入的数值范围进行限制，超出显示宽度的数值能正常存储和显示：










```sql
INSERT INTO test (num1) VALUES (12345);  -- 对于 INT(3) 也能正常插入
SELECT num1 FROM test;
+-------+
| num1  |
+-------+
| 12345 |  -- 正常显示，不会截断
+-------+
```






理解显示宽度的真正含义后，在设计表结构时，应根据实际的数据范围来选择合适的整数类型（如 `TINYINT`、`BIGINT` 等），而不是通过调整显示宽度来优化存储。



## Mysql中DoublewriteBuffer是什么作用是什么

### 1. 基本概念

#### 部分写失效（Partial Page Write）

- **问题描述**：当 MySQL 对数据页进行写操作时，如果在写入过程中发生了电源故障、系统崩溃等情况，可能会导致数据页只写入了一部分，即出现部分写失效问题。
- **产生原因**：InnoDB 数据页的大小通常为 16KB，而操作系统的写操作单位一般是 4KB。这就使得一次完整的数据页写入可能需要多次操作系统级别的写操作，从而增加了部分写失效的风险。

#### Doublewrite Buffer 的工作机制

- 写流程

  ：

  1. 当 InnoDB 要将数据页写入磁盘时，会先把数据页写入到内存中的 Doublewrite Buffer 区域。
  2. Doublewrite Buffer 是连续的物理存储区域，通常大小为 2MB，能够存储多个数据页。
  3. 接着，这些数据页会被顺序地写入到共享表空间（ibdata 文件）中的 Doublewrite 磁盘区域。
  4. 最后，数据页才会被写入到实际的数据文件（如 .ibd 文件）中对应的位置。

- **恢复机制**：当发生崩溃恢复时，InnoDB 会先检查 Doublewrite 区域中的数据页。如果发现数据文件中的页存在损坏，就会从 Doublewrite 区域中读取完整的数据页进行恢复。

### 2. 核心作用

#### 保障数据页完整性

- Doublewrite Buffer 能够确保在系统崩溃后，数据页不会处于部分更新的状态。因为所有的数据页更新都会先在 Doublewrite 区域中完整保存，即使写入过程中出现崩溃，也可以利用 Doublewrite 区域中的完整备份进行恢复。

#### 提升写性能

- 虽然看起来 Doublewrite 增加了写操作的次数，但由于 Doublewrite 区域是连续存储的，顺序写的性能要远高于随机写。而且，InnoDB 会批量将数据页从 Doublewrite Buffer 写入磁盘，进一步优化了写操作的性能。

#### 支持原子页写入

- 在没有 Doublewrite 的情况下，16KB 的数据页写入不是原子操作，可能会出现部分写失效。而 Doublewrite 通过先完整写入共享表空间，再写入实际数据文件的方式，实现了原子页写入。

### 3. 相关参数

#### `innodb_doublewrite`

- **作用**：控制是否启用 Doublewrite 功能，默认值为 `ON`。
- **禁用风险**：禁用该功能会显著提高数据页损坏的风险，因此不建议在生产环境中禁用。

#### `innodb_doublewrite_file`

- **作用**：指定 Doublewrite 文件的路径，从 MySQL 5.7 开始，该参数已被弃用，Doublewrite 数据会存储在系统表空间中。

#### `innodb_doublewrite_batch_size`

- **作用**：控制每次从 Doublewrite Buffer 写入磁盘的数据页数量，默认值为 512。

### 4. 性能影响与优化

#### 性能开销

- 启用 Doublewrite 会带来大约 5% - 10% 的写性能开销，这主要是因为需要进行两次写操作（先写 Doublewrite 区域，再写实际数据文件）。不过，这种开销换来了数据的高可靠性，通常是值得的。

#### 优化建议

- **使用 SSD 存储**：由于 SSD 的随机写性能较好，Doublewrite 带来的性能影响会相对较小。
- **调整刷新策略**：通过调整 `innodb_flush_log_at_trx_commit` 参数（如设置为 2），可以在一定程度上减少写操作的延迟。
- **监控 Doublewrite 活动**：

sql











```sql
-- 查看 Doublewrite 相关状态
SHOW ENGINE INNODB STATUS;

-- 关注以下指标：
-- doublewrite buffer entries：Doublewrite 缓冲区的条目数
-- bytes written to doublewrite buffer：写入 Doublewrite 缓冲区的字节数
-- pages written to doublewrite buffer：写入 Doublewrite 缓冲区的数据页数
-- doublewrite buffer bytes written：从 Doublewrite 缓冲区写入磁盘的字节数
-- doublewrite buffer pages written：从 Doublewrite 缓冲区写入磁盘的数据页数
```

### 5. 适用场景与限制

#### 适用场景

- **任何需要数据高可靠性的场景**：特别是在金融、电商等对数据完整性要求极高的领域。
- **使用机械硬盘（HDD）的环境**：由于 HDD 的随机写性能较差，部分写失效的风险更高，因此更需要 Doublewrite 保护。

#### 限制

- **仅适用于 InnoDB 引擎**：MyISAM 等其他存储引擎不支持 Doublewrite。
- **不替代备份**：Doublewrite 只能防止部分写失效，无法替代定期的全量和增量备份。
- **写密集型工作负载的性能影响**：在写密集型的场景下，Doublewrite 可能会成为性能瓶颈，需要根据实际情况权衡。

### 6. 与其他持久化机制的对比

| 机制                           | 主要作用                         | 性能影响            |
| ------------------------------ | -------------------------------- | ------------------- |
| Doublewrite Buffer             | 防止部分写失效，保障数据页完整性 | 中等（约 5% - 10%） |
| InnoDB 日志（Redo Log）        | 确保事务的持久性和崩溃恢复       | 较低                |
| Sync_binlog                    | 确保二进制日志同步到磁盘         | 较高                |
| Innodb_flush_log_at_trx_commit | 控制事务提交时日志的刷新策略     | 高（设置为 1 时）   |

### 7. 常见问题解答

#### Q1：如何判断 Doublewrite 是否正常工作？

- 可以通过 `SHOW ENGINE INNODB STATUS` 查看 Doublewrite 相关的统计信息，或者检查错误日志中是否有 Doublewrite 相关的警告。

#### Q2：禁用 Doublewrite 会有什么后果？

- 禁用后，InnoDB 将无法保证数据页的原子写入，在发生崩溃时可能会导致数据页损坏，需要依赖备份进行恢复。

#### Q3：如何降低 Doublewrite 的性能影响？

- 可以考虑使用更快的存储设备（如 SSD）、调整刷新策略，或者在非关键业务中适当降低 `innodb_flush_log_at_trx_commit` 的值。



## Mysql中如何解决深度分页问题

### 1. 问题原因

#### `LIMIT OFFSET` 的执行流程

- MySQL 会先扫描偏移量（`OFFSET`）之前的所有行，然后跳过这些行，再返回指定数量（`LIMIT`）的行。
- 当偏移量非常大时（例如 `LIMIT 1000000, 10`），即使只需要返回 10 行数据，MySQL 也需要扫描并丢弃前面的 100 万行数据，这会消耗大量的 CPU 和内存资源，导致查询速度极慢。

#### 示例说明

假设有一个包含 1000 万行数据的 `orders` 表：

sql











```sql
-- 执行时间可能长达数秒甚至更久
SELECT * FROM orders ORDER BY id LIMIT 1000000, 10;
```

### 2. 解决方案

#### 方案一：覆盖索引 + 子查询优化

- **原理**：利用覆盖索引直接获取需要的主键，避免回表操作，然后再通过主键关联原表获取完整数据。
- **示例**：

sql











```sql
-- 为 created_at 字段创建索引
CREATE INDEX idx_created_at ON orders (created_at);

-- 优化后的查询
SELECT o.* 
FROM orders o
JOIN (
    SELECT id 
    FROM orders 
    ORDER BY created_at 
    LIMIT 1000000, 10
) AS t 
ON o.id = t.id;
```

- **效果**：子查询只需扫描索引树，无需回表，大大减少了数据扫描量。

#### 方案二：书签记录法（推荐）

- **原理**：记录上次查询的最后一条记录的主键值，下次查询时直接从该位置开始。
- **示例**：

sql











```sql
-- 第一次查询（假设初始 id 为 0）
SELECT * FROM orders WHERE id > 0 ORDER BY id LIMIT 10;

-- 记录上次查询的最后一条记录的 id（例如 10）
-- 后续查询直接从该 id 开始
SELECT * FROM orders WHERE id > 10 ORDER BY id LIMIT 10;
```

- **适用场景**：适用于按主键或有序索引列分页的场景，如时间、自增 ID 等。

#### 方案三：预计算分页结果

- **原理**：对于固定的分页需求，提前计算并存储分页结果。

- 实现方式

  ：

  - 使用定时任务生成常用的分页数据。
  - 将分页结果缓存到 Redis 等缓存系统中。

- **示例**：










```python
# Python 伪代码示例
import redis
import pymysql

def precompute_pages():
    conn = pymysql.connect(host='localhost', user='root', password='', db='test')
    r = redis.Redis(host='localhost', port=6379, db=0)
    
    page_size = 10
    total_pages = 1000
    
    for page in range(1, total_pages + 1):
        offset = (page - 1) * page_size
        query = f"SELECT * FROM orders ORDER BY id LIMIT {offset}, {page_size}"
        with conn.cursor() as cursor:
            cursor.execute(query)
            results = cursor.fetchall()
            r.set(f"orders_page_{page}", str(results))
    
    conn.close()
```

#### 方案四：限制分页深度

- **原理**：在应用层限制用户可访问的最大页数，例如限制用户最多只能访问前 1000 页。
- **示例**：








```sql
-- 前端请求参数校验
if (page > 1000) {
    throw new Error("Page number exceeds the limit");
}
```

#### 方案五：分区表

- **原理**：将大表按时间或范围分成多个小表，减少单表数据量。
- **示例**：








```sql
-- 按年对 orders 表进行分区
CREATE TABLE orders (
    id INT,
    order_date DATE,
    amount DECIMAL(10, 2)
)
PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p2020 VALUES LESS THAN (2021),
    PARTITION p2021 VALUES LESS THAN (2022),
    PARTITION p2022 VALUES LESS THAN (2023)
);
```

- **查询优化**：










```sql
-- 只查询 2022 年的数据
SELECT * FROM orders PARTITION (p2022) ORDER BY id LIMIT 1000, 10;
```

#### 方案六：搜索引擎替代

- **适用场景**：对于全文检索或超大规模数据的分页需求，可使用 Elasticsearch 等搜索引擎替代。
- **示例**：







```sql
-- MySQL 查询
SELECT * FROM articles WHERE MATCH(content) AGAINST('keyword') LIMIT 10000, 10;

-- Elasticsearch 查询
GET /articles/_search
{
  "query": {
    "match": {
      "content": "keyword"
    }
  },
  "from": 10000,
  "size": 10
}
```

### 3. 性能对比

| 方案              | 数据量（万行） | 偏移量 | 查询时间 | 说明                     |
| ----------------- | -------------- | ------ | -------- | ------------------------ |
| LIMIT OFFSET      | 1000           | 100 万 | 10s+     | 性能随偏移量增大急剧下降 |
| 覆盖索引 + 子查询 | 1000           | 100 万 | 1s       | 避免回表，但仍需扫描索引 |
| 书签记录法        | 1000           | 100 万 | 0.1s     | 性能稳定，不受偏移量影响 |
| 预计算            | 1000           | 任意   | 0.01s    | 依赖缓存，实时性较差     |

### 4. 最佳实践建议

- **优先使用书签记录法**：对于按时间或自增 ID 分页的场景，书签记录法是性能最优的解决方案。
- **避免大偏移量查询**：通过业务设计限制用户访问深度，如电商网站通常只展示前 100 页结果。
- **结合覆盖索引**：在无法避免使用 `LIMIT OFFSET` 时，确保查询能利用覆盖索引，减少回表开销。
- **监控与调优**：定期分析慢查询日志，对深度分页查询进行针对性优化。

### 1. 核心原理

1. **利用覆盖索引**：先在索引树上完成排序和分页操作，快速定位目标行的主键。
2. **延迟回表**：只对最终需要返回的少量数据行进行回表操作，获取完整数据。
3. **减少扫描量**：避免直接对全量数据进行偏移量扫描，从而显著提升查询效率。

### 2. 实现步骤

#### 示例表结构

sql











```sql
CREATE TABLE orders (
    id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id INT,
    order_date DATETIME,
    amount DECIMAL(10, 2),
    status TINYINT,
    KEY idx_user_date (user_id, order_date DESC)
);
```

#### 传统分页（性能较差）

sql











```sql
-- 扫描 user_id=1 的前 1000000 行，丢弃 999990 行，返回 10 行
SELECT * FROM orders 
WHERE user_id = 1 
ORDER BY order_date DESC 
LIMIT 1000000, 10;  -- 可能需要数秒
```

#### 延迟分页优化

sql











```sql
-- 步骤1：通过覆盖索引快速定位主键
SELECT id 
FROM orders 
WHERE user_id = 1 
ORDER BY order_date DESC 
LIMIT 1000000, 10;  -- 仅扫描索引，速度快

-- 步骤2：根据主键批量回表获取完整数据
SELECT * 
FROM orders 
WHERE id IN (/* 步骤1返回的主键列表 */);
```

#### 合并为单条查询（更高效）

sql











```sql
SELECT o.* 
FROM orders o
JOIN (
    -- 子查询仅扫描索引树，避免回表
    SELECT id 
    FROM orders 
    WHERE user_id = 1 
    ORDER BY order_date DESC 
    LIMIT 1000000, 10
) AS t 
ON o.id = t.id;  -- 仅对最终的 10 行数据回表
```

### 3. 性能对比

| 查询方式           | 扫描行数 | 回表次数 | 执行时间（示例） |
| ------------------ | -------- | -------- | ---------------- |
| 传统分页           | 1000010  | 1000010  | 5.2s             |
| 延迟分页           | 1000010  | 10       | 0.3s             |
| 书签记录法（对比） | 10       | 10       | 0.01s            |

### 4. 适用场景

- 适用于

  ：

  - 必须使用 `LIMIT OFFSET` 进行分页，且偏移量较大的场景。
  - 查询条件和排序字段能构成覆盖索引（如示例中的 `(user_id, order_date)`）。
  - 数据量较大，传统分页性能瓶颈明显。

- 不适用

  ：

  - 没有合适的索引支持，导致全表扫描。
  - 分页深度极深（如超过百万级偏移量），此时仍建议使用书签记录法。

### 5. 优化建议

1. **确保索引覆盖**：创建包含查询条件和排序字段的复合索引，避免回表。







   ```sql
   -- 为示例表创建覆盖索引
   ALTER TABLE orders ADD INDEX idx_user_date (user_id, order_date DESC);
   ```

2. **限制分页深度**：结合业务逻辑限制最大可访问页数，如：











   ```sql
   -- 前端校验
   if (page > 1000) {
       throw new Error("Page number exceeds the limit");
   }
   ```

3. **结合书签记录法**：对于超深分页，可先用延迟分页获取初始数据，后续通过书签记录法继续分页：






   ```sql
   -- 初始查询（延迟分页）
   SELECT * FROM orders o
   JOIN (
       SELECT id FROM orders WHERE user_id = 1 ORDER BY order_date DESC LIMIT 1000, 10
   ) AS t ON o.id = t.id;
   
   -- 记录最后一条记录的 order_date（如 '2023-01-01 12:00:00'）
   -- 后续查询（书签记录法）
   SELECT * FROM orders 
   WHERE user_id = 1 
     AND order_date < '2023-01-01 12:00:00' 
   ORDER BY order_date DESC 
   LIMIT 10;
   ```

### 6. 注意事项

- **索引维护成本**：过多的覆盖索引会增加写操作的开销，需权衡读写比例。

- **偏移量陷阱**：即使使用延迟分页，超深偏移量（如 `LIMIT 1000000, 10`）仍需扫描大量索引数据，建议尽量避免。

- 复合索引顺序

  ：索引字段顺序需与查询条件和排序字段匹配，例如：







  ```sql
  -- 查询条件：WHERE user_id = 1 ORDER BY order_date DESC
  -- 正确的索引：(user_id, order_date) 或 (user_id, order_date DESC)
  ```

### 7. 与其他分页方法的对比

| 方法              | 优势                     | 劣势               | 适用场景               |
| ----------------- | ------------------------ | ------------------ | ---------------------- |
| 传统 LIMIT OFFSET | 实现简单                 | 深度分页性能极差   | 小数据量或浅分页       |
| 书签记录法        | 性能稳定，不受偏移量影响 | 需要记录上次位置   | 按时间 / ID 顺序分页   |
| 延迟分页          | 比传统分页快 10-100 倍   | 仍受偏移量影响     | 有覆盖索引的中深度分页 |
| 预计算 / 缓存     | 查询极快                 | 占用内存，实时性差 | 热点数据固定分页       |

### 总结

延迟分页是传统 `LIMIT OFFSET` 的优化升级版，通过延迟回表操作大幅提升了中深度分页的性能。但对于超大规模数据的分页需求，仍建议优先使用书签记录法或结合搜索引擎方案。在实际应用中，需根据业务场景选择最合适的分页策略，并通过 `EXPLAIN` 命令分析查询执行计划，持续优化索引设计。



## Mysql中limit10000000，10和limit10的执行速度是否相同

### 1. 核心差异

#### `LIMIT 10` 的执行流程

1. **扫描数据**：从索引或表中读取数据行。
2. **返回结果**：一旦找到 10 行数据，立即停止扫描并返回结果。
3. **时间复杂度**：通常为 **O(10)**，即与结果集大小成正比。

#### `LIMIT 10000000, 10` 的执行流程

1. **扫描数据**：从索引或表中逐行读取数据，**跳过前 1000 万行**。
2. **丢弃数据**：将前 1000 万行数据丢弃，不做任何处理。
3. **返回结果**：读取并返回接下来的 10 行数据。
4. **时间复杂度**：**O(10000010)**，即与 `OFFSET + LIMIT` 的总和成正比。

### 2. 性能对比示例

假设 `orders` 表有 1 亿行数据，且按 `id` 自增排序：

| 查询语句                                  | 扫描行数 | 执行时间（示例）    |
| ----------------------------------------- | -------- | ------------------- |
| `SELECT * FROM orders LIMIT 10`           | 10       | ~0.01s              |
| `SELECT * FROM orders LIMIT 10000000, 10` | 10000010 | ~10s （取决于硬件） |

### 3. 影响因素

#### 索引使用

- **有合适索引**：若查询能利用索引（如按主键或索引列排序），扫描速度会快很多，但仍需遍历 `OFFSET` 行。
- **无索引**：需全表扫描，性能会更差。

#### 数据分布

- **数据分散**：若数据分布在多个磁盘块或页中，读取 `OFFSET` 行时可能产生大量随机 I/O。
- **数据集中**：若数据集中存储，顺序读取效率较高。

#### 硬件性能

- **SSD**：随机读取性能较好，深度分页影响相对较小。
- **HDD**：随机 I/O 开销大，深度分页可能导致严重性能问题。

### 4. 执行计划差异

通过 `EXPLAIN` 命令可查看两者的执行计划差异：






```sql
-- LIMIT 10
EXPLAIN SELECT * FROM orders LIMIT 10;
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref  | rows | Extra       |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+
| 1  | SIMPLE      | orders | index | NULL          | PRIMARY | 8       | NULL | 10   | Using index |
+----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+

-- LIMIT 10000000, 10
EXPLAIN SELECT * FROM orders LIMIT 10000000, 10;
+----+-------------+--------+-------+---------------+---------+---------+------+---------+-------------+
| id | select_type | table  | type  | possible_keys | key     | key_len | ref  | rows    | Extra       |
+----+-------------+--------+-------+---------------+---------+---------+------+---------+-------------+
| 1  | SIMPLE      | orders | index | NULL          | PRIMARY | 8       | NULL | 10000010 | Using index |
+----+-------------+--------+-------+---------------+---------+---------+------+---------+-------------+
```

注意观察 `rows` 字段：

- `LIMIT 10` 只需扫描 10 行。
- `LIMIT 10000000, 10` 需扫描 10000010 行，即使最终只返回 10 行。

### 5. 为什么 MySQL 不直接跳到第 1000 万行？

- **索引结构限制**：B+ 树索引是有序的，但只能高效地从根节点开始遍历，无法直接定位到中间位置。
- **逻辑行 vs 物理行**：MySQL 中逻辑行（如按 `id` 排序）与物理存储位置可能不一致，需逐行遍历才能确定顺序。

### 6. 如何优化深度分页？

#### 方案一：书签记录法（推荐）










```sql
-- 记录上次查询的最大 id
SELECT * FROM orders WHERE id > 10000000 ORDER BY id LIMIT 10;
```

- **原理**：通过索引直接定位到起始位置，无需扫描中间行。
- **时间复杂度**：O (10)，与偏移量无关。

#### 方案二：延迟分页









```sql
SELECT * FROM orders 
WHERE id IN (
    SELECT id FROM orders ORDER BY created_at LIMIT 10000000, 10
);
```

- **原理**：先通过覆盖索引获取主键，再回表查询完整数据，减少回表次数。

#### 方案三：限制分页深度












```sql
-- 前端校验
if (page > 1000) {
    throw new Error("Page number exceeds the limit");
}
```

### 总结

- **执行速度**：`LIMIT 10000000, 10` 比 `LIMIT 10` 慢 **100 万倍以上**。

- 适用场景

  ：

  - `LIMIT 10`：适用于浅分页（如首页、前几页）。
  - `LIMIT 10000000, 10`：几乎不推荐使用，性能极差。

- **最佳实践**：避免大偏移量分页，改用书签记录法或搜索引擎方案。



## Mysql中Auto_increment列达到最大值时候发生什么

在 MySQL 中，当 `AUTO_INCREMENT` 列达到其数据类型的最大值时，后续的插入操作会根据存储引擎和 SQL 模式的不同而产生不同的行为。以下是详细说明：

### 1. **数据类型限制**

`AUTO_INCREMENT` 列必须是 **整数类型**（如 `TINYINT`, `INT`, `BIGINT` 等），每种类型都有固定的取值范围：

| 类型       | 无符号范围（UNSIGNED）          | 有符号范围                                              |
| ---------- | ------------------------------- | ------------------------------------------------------- |
| `TINYINT`  | 0 到 255                        | -128 到 127                                             |
| `SMALLINT` | 0 到 65,535                     | -32,768 到 32,767                                       |
| `INT`      | 0 到 4,294,967,295              | -2,147,483,648 到 2,147,483,647                         |
| `BIGINT`   | 0 到 18,446,744,073,709,551,615 | -9,223,372,036,854,775,808 到 9,223,372,036,854,775,807 |

当列值达到上限后，行为取决于存储引擎和 SQL 模式。

### 2. **InnoDB 引擎行为**

#### **严格 SQL 模式（默认）**

当 `AUTO_INCREMENT` 达到最大值后：

- **插入操作失败**：MySQL 会抛出错误 `ERROR 1062 (23000): Duplicate entry 'MAX_VALUE' for key 'PRIMARY'`。
- **事务回滚**：如果在事务中，整个事务会回滚。
- **值不会重置**：列值保持为最大值，不会循环到 0。

**示例**：







```sql
-- 创建表（TINYINT UNSIGNED 最大值为 255）
CREATE TABLE test (
    id TINYINT UNSIGNED AUTO_INCREMENT PRIMARY KEY
);

-- 插入 255
INSERT INTO test VALUES (255);

-- 尝试插入下一个值（隐式 AUTO_INCREMENT）
INSERT INTO test VALUES (NULL);  -- 报错：Duplicate entry '255'
```

#### **非严格 SQL 模式**

- **插入操作被静默忽略**：MySQL 会丢弃该插入语句，但不会报错。
- **数据丢失风险**：应用可能误以为插入成功，但实际上没有数据被添加。

### 3. **MyISAM 引擎行为**

- **插入操作失败**：与 InnoDB 相同，会抛出 `Duplicate entry` 错误。
- **表锁风险**：MyISAM 不支持事务，错误可能导致表锁死，需手动干预。

### 4. **如何处理达到上限的情况**

#### **方案 1：提前扩容数据类型**

在设计表时，预估数据量并选择足够大的类型（如 `BIGINT`）：









```sql
-- 使用 BIGINT UNSIGNED（最大值约 1.8e19）
CREATE TABLE users (
    id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100)
);
```

#### **方案 2：手动重置 AUTO_INCREMENT 值**

如果确定某些 ID 已不再使用，可以重置计数器：







```sql
-- 将 AUTO_INCREMENT 重置为 1（需谨慎，可能导致主键冲突）
ALTER TABLE test AUTO_INCREMENT = 1;
```

#### **方案 3：水平分表（Sharding）**

当单表数据量极大时，将数据分散到多个表中：









```sql
-- 按范围分表（如 user_0, user_1, ...）
CREATE TABLE user_0 (id BIGINT PRIMARY KEY, ...);
CREATE TABLE user_1 (id BIGINT PRIMARY KEY, ...);
```

#### **方案 4：使用 UUID 替代自增 ID**

使用 `UUID()` 或 `UUID_SHORT()` 生成唯一标识符：









```sql
-- 使用 UUID
CREATE TABLE logs (
    id CHAR(36) PRIMARY KEY DEFAULT UUID(),
    message TEXT
);
```

### 5. **监控 AUTO_INCREMENT 使用情况**

通过 `information_schema` 查看当前自增列的使用情况：









```sql
SELECT 
    TABLE_NAME,
    AUTO_INCREMENT,
    COLUMN_TYPE
FROM 
    information_schema.TABLES
WHERE 
    TABLE_SCHEMA = 'your_database'
    AND AUTO_INCREMENT IS NOT NULL;
```

### 6. **最佳实践**

1. **优先使用 `BIGINT UNSIGNED`**：避免未来因数据增长导致的问题。
2. **启用严格 SQL 模式**：避免数据丢失风险。
3. **定期监控自增列使用情况**：提前发现潜在问题。
4. **设计可扩展的架构**：如分库分表、分布式 ID 生成器（如 Snowflake）。

### 总结

当 `AUTO_INCREMENT` 达到最大值时，MySQL 会阻止插入新数据并报错，因此需要在设计阶段选择合适的数据类型或采用替代方案（如 UUID、分表）。及时监控自增列的使用情况可以避免生产环境出现此类问题。



## Mysql中的金额数字用什么类型来存储
### 1. **DECIMAL 类型（推荐）**

`DECIMAL` 是存储金额的**首选类型**，因为它能**精确表示小数**，避免浮点数误差。

#### **语法**









```sql
DECIMAL(M, D)
```

- `M`：总位数（精度，默认 10）。
- `D`：小数位数（标度，默认 0）。

#### **示例**









```sql
-- 存储金额，最大 10 位数字，2 位小数（如 99999999.99）
amount DECIMAL(10, 2)

-- 存储更大金额（如 9999999999.99）
amount DECIMAL(12, 2)
```

#### **特点**

- **精确性**：不会出现浮点数（如 `FLOAT`、`DOUBLE`）的舍入误差。
- **存储空间**：每 9 位数字占用 4 字节，不足 9 位按比例计算（如 4 位占 2 字节）。
- **性能**：计算速度略慢于整数类型，但远高于浮点数。

### 2. **整数类型（INT/BIGINT）**

存储**最小货币单位**（如 “分”），避免小数计算。

#### **示例**





```sql
-- 以“分”为单位存储金额（最大约 21 亿分，即 2100 万元）
amount INT UNSIGNED

-- 存储更大金额（最大约 922 亿亿元）
amount BIGINT UNSIGNED
```

#### **特点**

- **精确性**：完全避免小数误差。
- **性能**：计算速度最快。
- **应用层转换**：需在代码中手动转换为 “元”（如 `amount / 100`）。

### 3. **FLOAT/DOUBLE（不推荐）**

`FLOAT` 和 `DOUBLE` 使用二进制存储，存在**舍入误差**，**不适合存储金额**。

#### **示例**






```sql
-- 不推荐：可能导致精度丢失（如 0.01 存储为 0.00999999）
amount FLOAT
```

#### **问题**







```sql
-- 示例：浮点数误差
SELECT 0.01 + 0.02;  -- 可能返回 0.030000000000000004
```

### 4. **选择建议**

| 场景                 | 推荐类型          | 示例                  |
| -------------------- | ----------------- | --------------------- |
| 常规金额（如人民币） | `DECIMAL(10, 2)`  | 99999999.99 元        |
| 超大型金额           | `DECIMAL(19, 4)`  | 9999999999999.9999 元 |
| 高性能计算           | `BIGINT UNSIGNED` | 以 “分” 为单位存储    |
| 仅整数金额           | `INT UNSIGNED`    | 以 “元” 为单位存储    |

### 5. **最佳实践**

1. **优先使用 `DECIMAL`**：适合绝大多数业务场景，无需手动转换。
2. **控制精度**：根据业务需求设置合适的 `M` 和 `D`（通常 `DECIMAL(10, 2)` 足够）。
3. **避免浮点数**：除非对精度要求极低（如科学计算）。
4. **考虑本地化**：不同货币的最小单位可能不同（如日元无小数）。
5. **索引优化**：对经常用于计算或排序的金额列添加索引。

### 6. **示例表结构**





```sql
CREATE TABLE orders (
    id INT PRIMARY KEY AUTO_INCREMENT,
    amount DECIMAL(10, 2) NOT NULL,  -- 金额，精确到分
    currency VARCHAR(3) NOT NULL,    -- 货币代码（如 CNY, USD）
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 7. **性能对比**

| 类型            | 存储空间 | 计算速度 | 精度风险 |
| --------------- | -------- | -------- | -------- |
| `DECIMAL(10,2)` | 8 字节   | 中等     | 无       |
| `BIGINT`        | 8 字节   | 最快     | 无       |
| `FLOAT`         | 4 字节   | 快       | 高       |

### 总结

- **推荐方案**：使用 `DECIMAL(10, 2)` 存储金额，平衡精度与性能。
- **高性能方案**：使用 `BIGINT` 存储最小货币单位（如 “分”），但需应用层转换。
- **避免浮点数**：任何对精度敏感的场景都不应使用 `FLOAT`/`DOUBLE`。


## 为什么不推荐使用存储过程

1. **性能问题**：存储过程在数据库服务器上执行，可能导致网络延迟和额外的开销。
2. **复杂性**：编写和维护存储过程可能比直接使用 SQL 更复杂。
3. **可移植性**：存储过程的语法和行为在不同数据库之间可能有所不同。
4. **可维护性**：存储过程的修改可能涉及到多个地方，增加维护成本。
5. **安全性**：存储过程可能包含 SQL 注入等安全风险。


## 如何实现数据库的不停服迁移

### **1. 迁移前的准备工作**

#### **1.1 评估与规划**

- **数据库评估**：分析源库的表结构、数据量、QPS/TPS、索引、存储引擎等。
- **目标库选型**：选择与源库兼容的数据库版本和存储引擎（如 MySQL 5.7 → 8.0）。
- **容量规划**：确保目标库硬件资源（CPU、内存、磁盘）足够支撑业务负载。

#### **1.2 架构设计**

- **双写方案**：在迁移期间，业务同时向源库和目标库写入数据，确保数据一致性。
- **读写分离**：迁移后，逐步将读请求切至目标库，写请求仍保留在源库。


## 什么是数据库的逻辑删除和物理删除
### **1. 逻辑删除**
#### **定义**
逻辑删除是指在数据库表中添加一个标记字段（通常是 `deleted_at` 或 `is_deleted`），用于标记数据的删除状态。
#### **优点**
- **数据保留**：逻辑删除不会真正删除数据，而是标记为已删除，数据仍然保留在数据库中。
- **数据恢复**：可以通过查询已删除的数据来恢复数据。
- **数据安全**：逻辑删除可以防止数据被物理删除，提高数据安全性。
#### **缺点**
- **查询性能**：查询已删除的数据时，需要额外的查询条件。
- **数据量**：逻辑删除会增加数据库的存储量，可能导致存储空间浪费。
### **2. 物理删除**
#### **定义**
物理删除是指直接从数据库表中删除数据。
#### **优点**
- **数据安全**：物理删除不会保留已删除的数据。
- **数据量**：物理删除不会增加数据库的存储量。
#### **缺点**
- **数据丢失**：物理删除后无法恢复数据。
- **数据恢复**：无法通过查询已删除的数据来恢复数据。
### **3. 选择逻辑删除还是物理删除**
- **数据安全**：如果数据安全性是最重要的，应该选择逻辑删除。
- **数据恢复**：如果需要恢复已删除的数据，应该选择逻辑删除。
- **性能**：如果数据量不大，且不需要恢复已删除的数据，应该选择物理删除。
- **存储空间**：如果数据量较大，且需要频繁删除数据，应该选择逻辑删除。
### **4. 总结**
逻辑删除和物理删除是两种常用的数据库删除方式，选择哪种方式取决于业务需求和数据安全性要求。